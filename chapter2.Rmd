# RStudio Exercise 2

*This week datawrangling, regression and model validation have been central in this course.*

###1. Read the data and explore dimensions and summaries

```{r}
data_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/learning2014.txt"
lrn14 <-read.table(data_url, sep=",", header = TRUE)
dim(lrn14)
str(lrn14)

```

The data consists of 166 students with the following 7 variables:


1. **Gender:** 1 = Male and 2 = Female.  
2. **Age:** The age of the students in years.  
3. **Attitude:** Global attitude toward statistics as a mean of various variables.  
4. **Deep:** Parameter calculated as the mean of various variables from the deep learning questions.  
5. **Stra:** Parameter calculated as the mean of various variables from the strategic learning questions.  
6. **Surf:** Parameter calculated as the mean of various variables from the surface learning questions.  
7. **Points:** Maximal exam points.


###2. Summary of the variables and grafical overview

```{r}
summary(lrn14)
```


The group consists of 110 females and 56 males. Their median age is 22 years with range [17-55].  The students got a median of 3.2 points with range [1.4-5] for attitude, a median of 3.7 with range [1.6-4.9] for deep learning approach (deep), a median of 3.2 with range [1.3-5] for strategic learning approach (stra), a median of 2.9 with range [1.6-4.3] for surface learning approach (surf) and a median of 23 points (points) for the exam with range [7-33].

```{r}
pairs(lrn14[-1], col=lrn14$gender)
```


The grafical summary above shows the relationship between the values of the different measurements with bivariable scatter plots. The colour of the scatter points shows males as black dots and females as red dots.

```{r}
library(GGally)
library(ggplot2)
p <- ggpairs(lrn14, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
```

The grafic above displays in more different ways the data than the bivariable scatterplot. The distribution of the results is normal, except for the parameters age and deep which are respectively right-skewed and left-skewed. Within the material is the relationship between the attitude toward statistics and final exam points the strongest with a positive correlation of 0,4 for both sexes. The second most strong correlation is seen between deep learning approach and surface learning approach. This correlation of 0,6 is negative and specific for the male group, for both groups the correlation is -0,3.

###3. Regression model

```{r}
model_1 <-lm(points ~ attitude + stra + deep, data = lrn14)
summary(model_1)
```

In the first model there is a clear significance with low p-value for attitude. The "deep"-variable will be removed and the model will be tested again.

```{r}
model_2 <-lm(points ~ attitude + stra, data = lrn14)
summary(model_2)
```

In this model is attitude again the only significant variable, that explains changes in the exam points. The model will be runned again without the "stra"-variable.

```{r}
model_3 <-lm(points ~ attitude, data = lrn14)
summary(model_3)
```


###4. Interpretation and explanation of model parameters

**Residuals:** The residuals show the difference between the values predicted by the model and the actual values. A good predictive value should have a symmetric distribution of the residuals.  
**Coefficients:** The *intercept* is the expected mean value of Y when all X=0. The *attitude's* slope is estimated at 3.53, meaning that the maximum test points change with 3,5 points for every point of attitude change. *Standard Error* should fall within plus/minus 2 times the standard error of the regression from the regression line within a 95% prediction interval.The *Pr(>t)* acronym found in the model output relates to the probability of observing any value equal or larger than t. A small p-value indicates that it is unlikely we will observe a relationship between the predictor (attitude) and response (points) variables due to chance. Three asterisks represent a highly significant p-value.  
**Residual standard error:** Residual Standard Error is measure of the quality of a linear regression fit. The Residual Standard Error is the average amount that the response (points) will deviate from the true regression line.  
**R-squared** statistic provides a measure of how well the model is fitting the actual data. A value of null means that the variable doesn't explain the variance in the dependent; 1 means the total opposite. In this case roughly 19 % of the variance found in the response variable (points) can be explained by the predictor variable (attitude).  
**F-statistic** is a good indicator whether there is a relationship between the variable and the dependent value. The further away from 0 the better the reliability of the test and the possibilty to reject the H0 (Noll hypothesis: " There is no relationship between attitude and points.").

###5. Diagnostic plots: Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage

```{r}
plot(model_3, which = c(1:2, 5))
```

**Residuals vs Fitted values:** In this plot there is a linear relationship between the predictor variables and the outcome variables, confirming the assumption that there are no non-linear relationships between the parameters in the test and the linear model fits well for this data. Some outliers can be observed in the model.  
**Normal Q-Q:** The relationship between the theoretical and  standarized quantiles is mainly following a straight line, confirming that the choice for a linear model was the correct one. In the lower left corner there are some outliers observed.  
**Residuals vs Leverage:** Some outliers could be identified in the *residuals vs fitted values*, but they do not show high leverage on the observations and the model can be used to discribe the relationship between 'attitude' and 'points'. The outliers in this case can be kept in the model. If they were due to possible registration errors in the data and had a high leverage on the model, they could be removed.